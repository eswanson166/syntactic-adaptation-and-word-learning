---
title: "Experiment 1 data cleaning"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(readr)
```

## Loading data

We first load the individual participant files and then combine them into one dataset.
```{r}
dir <- "../data/raw_data"
participant_files <- list.files(path = dir, full.names = TRUE)
et_data <- plyr::ldply(participant_files, function(x) {
  read_csv(x)
})
et_data <- et_data %>% mutate_if(is.character, factor)
```

## Cleaning data

We take out data points that were simply used to test the experiment (and therefore had the experimenter's name in the comments).
```{r}
et_data <- et_data %>% filter(!grepl("Elizabeth",comments))
```

We also remove participants who do not list French as their native language.
```{r}
unique(et_data$language)
et_data <- et_data %>% filter(grepl("(fran(รง|c)ais(e?))|(france)|(french)", 
                                    ignore.case=TRUE,language))
unique(et_data$language)
```

We also take out participants who did not complete at least 50% of trials.
```{r}
# add a column with number of trials completed
et_data <- et_data %>% group_by(participant_id) %>% 
  mutate(trials_completed = length(unique(trial_no)))

# filter out participants who have completed <5 trials
et_data <- et_data %>% filter(trials_completed >= 5)
```

We remove trials with a window width of less than 1280 pixels.
```{r}
et_data <- et_data %>% filter(screenW >= 1280)
```


## Time of the video

We need to add a new column that gives the time since the beginning of the trial.
```{r}
# make a column with the datapoint number during the trial
et_data <- et_data %>% group_by(participant_id, trial_no) %>%
  mutate(data_point_no = seq_along(time))

# make a column with starting time of the trial
et_data <- et_data %>% group_by(participant_id, trial_no) %>%
  mutate(starting_time = time[data_point_no == 1])

# make a column with the time since the beginning of the trial
et_data$time <- as.numeric(et_data$time)
et_data <- et_data %>% mutate(time_since_trial_start = time - starting_time)
```


We also add a column stating what part of the video is playing at that data point.
```{r}
et_data$video_stage <- "left_preview"
et_data$video_stage[et_data$time_since_trial_start > et_data$pre1_time_from_start] <- "right_preview"
et_data$video_stage[et_data$time_since_trial_start > et_data$pre2_time_from_start] <- "contrast"
et_data$video_stage[et_data$time_since_trial_start > et_data$contrast_time_from_start] <- "audio"
et_data$video_stage[et_data$time_since_trial_start > et_data$audio_time_from_start] <- "event"
```


## Adding AOIs

We also add the AOIs for this experiment. Note that this is based on the size of the specific videos we used (400 pixels wide and 226 pixels high). Since the Webgazer measures are messy, we add 100 pixels of padding around each edge of the video.

First, add the coordinates for the video on the left (video 1).
```{r}
margin_left <- (et_data$screenW - 1280)/2
margin_top <- (et_data$screenH - 226)/2

et_data$vid1_left_x <- margin_left - 100
et_data$vid1_right_x <- margin_left + 400 + 100
et_data$top_y <- margin_top - 100
et_data$bottom_y <- margin_top + 226 + 100
```

Then, add the coordinates for the video on the right (video 2). The y coordinates for the video on the right are the same as the video on the left.
```{r}
et_data$vid2_left_x <- margin_left + 880 - 100
et_data$vid2_right_x <- margin_left + 1280 + 100
```

We add a column that is TRUE if the participant was looking at the video on the left.
```{r}
et_data$look_left_video <- FALSE
et_data$look_left_video[(et_data$vid1_left_x <= et_data$x) & (et_data$x <= et_data$vid1_right_x) & (et_data$top_y <= et_data$y) & (et_data$y <= et_data$bottom_y)] <- TRUE
```

We add a column that is TRUE if the participant was looking at the video on the right.
```{r}
et_data$look_right_video <- FALSE
et_data$look_right_video[(et_data$vid2_left_x <= et_data$x) & (et_data$x <= et_data$vid2_right_x) & (et_data$top_y <= et_data$y) & (et_data$y <= et_data$bottom_y)] <- TRUE
```


## Looking at action or object video

We add a column that states whether the action video was on the left or the right.
```{r}
et_data$action_video <- "right"
et_data$action_video[grepl("verb", et_data$left_video)] <- "left"
```

We also add a column that is TRUE if the participant was looking at the action video.
```{r}
et_data$look_action_video <- FALSE
et_data$look_action_video[et_data$look_left_video == TRUE & et_data$action_video == "left"] <- TRUE
et_data$look_action_video[et_data$look_right_video == TRUE & et_data$action_video == "right"] <- TRUE
```

We also add a column that is TRUE if the participant was looking at the noun video. (Since we discount looks that are not to either video, this is not really needed for this analysis, but would be useful if we did not discount those looks.)
```{r}
et_data$look_object_video <- FALSE
et_data$look_object_video[et_data$look_left_video == TRUE & et_data$action_video == "right"] <- TRUE
et_data$look_object_video[et_data$look_right_video == TRUE & et_data$action_video == "left"] <- TRUE
```


## Previous look

Finally, we add columns that indicate whether the participant was looking at the action video or the noun video on the previous look.
```{r}
et_data <- et_data %>% group_by(participant_id, trial_no) %>%
  mutate(previous_look_action_video = lag(look_action_video),
         previous_look_object_video = lag(look_object_video))
```

## Write the data to a .csv file

Now the data is ready for visualization and analysis. We write it to a .csv file. Note that this is an extremely large file, so it is stored in a .zip file on GitHub. 
```{r}
write.csv(et_data, "../data/clean_data.csv")
```

